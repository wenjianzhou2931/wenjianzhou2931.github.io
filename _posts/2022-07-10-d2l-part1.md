---
layout: article
title: Notes taken while reading 'Dive into Deep Learning'
---

## Key components while encountering with ML problems

1. The **data** that we can learn from.
2. A **model**  of how to transform the data.
3. An **objective** function that quantifies how well (or badly) the model is doing.
4. An **algorithm** to adjust the model's parameters to optimize the objective function.

### Data

Each **example** (or **data point, data instance, sample**) consists of a set of attributes called **features** (or **covariates**). The thing to predict is a special attribute that is designated as the **label** (or **target**).

### Model

By **model**, we denote the computational machinery for ingesting data of one type, and spitting out predictions of a possibly different type.

### Objective Functions

**Objective functions** are formal measures of how good (or bad) our models are. We usually define objective functions so that lower is better.

Because lower is better, these functions are sometimes called **loss functions**.

The most common loss function is **squared error**, the square of the difference between the prediction and the ground-truth.

For classification, the most common objective is to minimize **error rate**, which is the fraction of examples on which our predictions disagree with the ground truth.

Objectives like squared error are easy to optimize, others like error rate are difficult to optimize directly. In these cases, it is common to optimize a **surrogate objective** (代理目标).

We will split the available data into two partitions: the **training dataset** (or **training set**, for fitting model parameters) and the **test dataset** (or **test set**, which is held out for evaluation).

When a model performs well on the training set but fails to generalize to unseen data, we say that it is **overfitting**.

### Optimization Algorithms

Once we have got all the 3 things above, we need an algorithm capable of searching for the best parameter s for minimizing the loss function.

Popular algorithms for deep learning are based on an approach called **gradient descent**.