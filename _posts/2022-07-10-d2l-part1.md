---
layout: article
title: Notes taken while reading 'Dive into Deep Learning'
---

# Key components while encountering with ML problems

1. The **data** that we can learn from.
2. A **model**  of how to transform the data.
3. An **objective** function that quantifies how well (or badly) the model is doing.
4. An **algorithm** to adjust the model's parameters to optimize the objective function.

## Data

Each **example** (or **data point, data instance, sample**) consists of a set of attributes called **features** (or **covariates**). The thing to predict is a special attribute that is designated as the **label** (or **target**).

## Model

By **model**, we denote the computational machinery for ingesting data of one type, and spitting out predictions of a possibly different type.

## Objective Functions

**Objective functions** are formal measures of how good (or bad) our models are. We usually define objective functions so that lower is better.

Because lower is better, these functions are sometimes called **loss functions**.

The most common loss function is **squared error**, the square of the difference between the prediction and the ground-truth.

For classification, the most common objective is to minimize **error rate**, which is the fraction of examples on which our predictions disagree with the ground truth.

Objectives like squared error are easy to optimize, others like error rate are difficult to optimize directly. In these cases, it is common to optimize a **surrogate objective** (代理目标).

We will split the available data into two partitions: the **training dataset** (or **training set**, for fitting model parameters) and the **test dataset** (or **test set**, which is held out for evaluation).

When a model performs well on the training set but fails to generalize to unseen data, we say that it is **overfitting**.

## Optimization Algorithms

Once we have got all the 3 things above, we need an algorithm capable of searching for the best parameter s for minimizing the loss function.

Popular algorithms for deep learning are based on an approach called **gradient descent**.

# Kinds of Machine Learning Problems

## Supervised Learning

Supervised learning addresses the task of predicting labels given input features. The goal is to produce a model that maps any input to a label prediction.

### Regression

Regression models address *how many?* questions.

The output is one numerical value, like a fair market value of a house, given some features of some houses as the dataset.

### Classification

Classification models address *which one?* questions.

The simplest form of classification is when there are only two classes, a problem which we call **binary classification**. For example, our dataset could consist of images of animals and our labels might be the classes **{cat, dog}**.

When we have more than two possible classes, we call the problem **multiclass classification**.

The common loss function for classification problems is called **cross-entropy**.

Classification can get much more complicated than just binary, multiclass, or even multi-label classification. Like **hierarchical classification**.

### Tagging

